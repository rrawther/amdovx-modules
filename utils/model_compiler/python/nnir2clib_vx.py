# Copyright (c) 2018 Advanced Micro Devices, Inc. All rights reserved.
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.

import os, sys, struct
import datetime, pytz
from nnir import *

tensor_type_nnir2openvx = {
    'F032' : 'VX_TYPE_FLOAT32',
    'F016' : 'VX_TYPE_FLOAT16',
    'U016' : 'VX_TYPE_UINT16',
    'I016' : 'VX_TYPE_INT16',
    'U008' : 'VX_TYPE_UINT8'
}

def generateLicenseForCPP(f):
        f.write( \
"""/*
MIT License

Copyright (c) 2018 Advanced Micro Devices, Inc. All rights reserved.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
*/

/* This file is generated by nnir2clib_vx.py on %s */
""" % (datetime.datetime.now(tz=pytz.timezone('America/Los_Angeles')).isoformat()))

def generateLicenseForScript(f):
        f.write( \
"""################################################################################
#
# MIT License
#
# Copyright (c) 2018 Advanced Micro Devices, Inc.
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
#
################################################################################

# This file is generated by nnir2clib_vx.py on %s
""" % (datetime.datetime.now(tz=pytz.timezone('America/Los_Angeles')).isoformat()))

def generateCMakeFiles(graph,outputFolder):
    fileName = outputFolder + '/CMakeLists.txt'
    print('creating ' + fileName + ' ...')
    with open(fileName, 'w') as f:
        generateLicenseForScript(f)
        f.write( \
"""
cmake_minimum_required (VERSION 2.8)
project (annmodule)
set (CMAKE_CXX_STANDARD 11)
list(APPEND CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake)
find_package(OpenCL REQUIRED)
find_package(OpenCV QUIET)
include_directories (${OpenCL_INCLUDE_DIRS} ${OpenCL_INCLUDE_DIRS}/Headers )
include_directories (/opt/rocm/include)
link_directories    (/opt/rocm/lib)
list(APPEND SOURCES idamodule.cpp)
add_library(${PROJECT_NAME} SHARED ${SOURCES})
set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -msse4.2 -std=c++11")
target_link_libraries(${PROJECT_NAME} openvx vx_nn pthread)

add_executable(infdeploy infdeploy.cpp)
if (OpenCV_FOUND)
  target_compile_definitions(infdeploy PUBLIC ENABLE_OPENCV=1)
  include_directories(${OpenCV_INCLUDE_DIRS})
  target_link_libraries(infdeploy ${OpenCV_LIBRARIES})
else(OpenCV_FOUND)
  target_compile_definitions(infdeploy PUBLIC ENABLE_OPENCV=0)
endif(OpenCV_FOUND)
target_link_libraries(infdeploy openvx vx_nn pthread ${PROJECT_NAME})

""")
    if not os.path.isdir(outputFolder + '/cmake'):
        os.mkdir(outputFolder + '/cmake')
    fileName = outputFolder + '/cmake/FindOpenCL.cmake'
    print('creating ' + fileName + ' ...')
    with open(fileName, 'w') as f:
        generateLicenseForScript(f)
        f.write( \
"""
find_path(OPENCL_INCLUDE_DIRS
    NAMES OpenCL/cl.h CL/cl.h
    HINTS
    ${OPENCL_ROOT}/include
    $ENV{AMDAPPSDKROOT}/include
    PATHS
    /usr/include
    /usr/local/include
    /opt/rocm/opencl/include
    DOC "OpenCL header file path"
    )
mark_as_advanced( OPENCL_INCLUDE_DIRS )

if("${CMAKE_SIZEOF_VOID_P}" EQUAL "8")
    find_library( OPENCL_LIBRARIES
        NAMES OpenCL
        HINTS
        ${OPENCL_ROOT}/lib
        $ENV{AMDAPPSDKROOT}/lib
        DOC "OpenCL dynamic library path"
        PATH_SUFFIXES x86_64 x64 x86_64/sdk
        PATHS
        /usr/lib
        /opt/rocm/opencl/lib
        )
else( )
    find_library( OPENCL_LIBRARIES
        NAMES OpenCL
        HINTS
        ${OPENCL_ROOT}/lib
        $ENV{AMDAPPSDKROOT}/lib
        DOC "OpenCL dynamic library path"
        PATH_SUFFIXES x86 Win32

        PATHS
        /usr/lib
        )
endif( )
mark_as_advanced( OPENCL_LIBRARIES )

include( FindPackageHandleStandardArgs )
find_package_handle_standard_args( OPENCL DEFAULT_MSG OPENCL_LIBRARIES OPENCL_INCLUDE_DIRS )

set(OpenCL_FOUND ${OPENCL_FOUND} CACHE INTERNAL "")
set(OpenCL_LIBRARIES ${OPENCL_LIBRARIES} CACHE INTERNAL "")
set(OpenCL_INCLUDE_DIRS ${OPENCL_INCLUDE_DIRS} CACHE INTERNAL "")

if( NOT OPENCL_FOUND )
    message( STATUS "FindOpenCL looked for libraries named: OpenCL" )
endif()
""")

def generateModuleH(graph,fileName):
    print('creating ' + fileName + ' ...')
    with open(fileName, 'w') as f:
        generateLicenseForCPP(f)
        f.write( \
"""
#ifndef included_file_idemodule_h
#define included_file_idemodule_h


////
// ide_handle_t (inference_deployment_engine_handle)
//
typedef struct ide_handle_t {
    vx_context  context;
    vx_graph    graph;
    size_t      input_dims[4], output_dims[4];
    void*       input_tensor;
    void*       output_tensor;
    bool        scheduled;
} * ide_handle;

////
// Inference Deployment Engine(ide) functions
""")
        for tensor in graph.inputs:
            f.write( \
"""//   %s -- dims[] = { %s } (input)
""" % (tensor.name, ', '.join([str(v) for v in reversed(tensor.shape)])))
        for tensor in graph.outputs:
            f.write( \
"""//   %s -- dims[] = { %s, } (output)
""" % (tensor.name, ', '.join([str(v) for v in reversed(tensor.shape)])))
        f.write( \
"""//
// enum for return codes
enum ide_status_e {
    IDE_STATUS_MIN                      = -25,/*!< \brief Indicates the lower bound of status codes. Used for bounds checks only. */
    /* add new codes here */
    IDE_ERROR_INVALID_TYPE               = -17,/*!< \brief Indicates that the supplied type parameter is incorrect. */
    IDE_ERROR_INVALID_VALUE              = -16,/*!< \brief Indicates that the supplied parameter has an incorrect value. */
    IDE_ERROR_INVALID_DIMENSION          = -15,/*!< \brief Indicates that the supplied parameter is too big or too small in dimension. */
    IDE_ERROR_INVALID_FORMAT             = -14,/*!< \brief Indicates that the supplied parameter is in an invalid format. */
    IDE_ERROR_INVALID_LINK               = -13,/*!< \brief Indicates that the link is not possible as specified. The parameters are incompatible. */
    IDE_ERROR_INVALID_REFERENCE          = -12,/*!< \brief Indicates that the reference provided is not valid. */
    IDE_ERROR_INVALID_PARAMETERS         = -10,/*!< \brief Indicates that the supplied parameter information does not match the API definition. */
    IDE_ERROR_OPTIMIZED_AWAY             = -9,/*!< \brief Indicates that the object refered to has been optimized out of existence. */
    IDE_ERROR_NO_MEMORY                  = -8,/*!< \brief Indicates that an internal or implicit allocation failed. Typically catastrophic.*/
    IDE_ERROR_NO_RESOURCES               = -7,/*!< \brief Indicates that an internal or implicit resource can not be acquired (not memory). This is typically catastrophic. */
    IDE_ERROR_NOT_COMPATIBLE             = -6,/*!< \brief Indicates that the attempt to link two parameters together failed due to type incompatibilty. */
    IDE_ERROR_NOT_ALLOCATED              = -5,/*!< \brief Indicates to the system that the parameter must be allocated by the system.  */
    IDE_ERROR_NOT_SUFFICIENT             = -4,/*!< \brief Indicates that the given graph has failed verification due to an insufficient number of required parameters, which cannot be automatically created. Typically this indicates required atomic parameters. \see vxVerifyGraph. */
    IDE_ERROR_NOT_SUPPORTED              = -3,/*!< \brief Indicates that the requested set of parameters produce a configuration that cannot be supported. */
    IDE_ERROR_NOT_IMPLEMENTED            = -2,/*!< \brief Indicates that the requested API is missing. */
    IDE_FAILURE                          = -1,/*!< \brief Indicates a generic error code, used when no other describes the error. */
    IDE_SUCCESS                          =  0,/*!< \brief No error. */
}

typdef int ide_status

#ifndef IDE_API_ENTRY
#define IDE_API_ENTRY
#endif
#ifndef IDE_API_CALL
#if defined(_WIN32)
#define IDE_API_CALL __stdcall
#else
#define IDE_API_CALL
#endif
#endif

//
extern "C" IDE_API_ENTRY int             IDE_API_CALL ideQueryInference(char **inp_name, size_t *inp_dims, char **out_name, size_t *out_dims);
extern "C" IDE_API_ENTRY int             IDE_API_CALL ideSetBackend(const char *nameBackend);
extern "C" IDE_API_ENTRY ide_handle      IDE_API_CALL ideCreateInference(void *input, void *output, const char * binaryFilename);
extern "C" IDE_API_ENTRY int             IDE_API_CALL ideSetInput(ide_handle handle, void * inp_tensor_mem);
extern "C" IDE_API_ENTRY int             IDE_API_CALL ideGetOutput(ide_handle handle, void * out_tensor_mem);
extern "C" IDE_API_ENTRY int             IDE_API_CALL ideProcessInference(ide_handle handle, int num_iterations = 1);
extern "C" IDE_API_ENTRY int             IDE_API_CALL ideScheduleInference(ide_handle handle);
extern "C" IDE_API_ENTRY int             IDE_API_CALL ideWaitForCompletion(ide_handle handle);
extern "C" IDE_API_ENTRY int             IDE_API_CALL ideReleaseInference(ide_handle handle);

#endif
""")

def generateModuleCPP(graph,fileName):
    print('creating ' + fileName + ' ...')
    with open(fileName, 'w') as f:
        generateLicenseForCPP(f)
        if len(graph.inputs) != 1 or len(graph.outputs) != 1:
            f.write( \
"""
#include "idemodule.h"

IDE_API_ENTRY ide_status IDE_API_CALL ideQueryInference(char **inp_name, size_t *inp_dims, char **out_name, size_t *out_dims)
{
    return IDE_FAILURE;
}

IDE_API_ENTRY ide_status IDE_API_CALL ideSetBackend(const char *nameBackend)
{
    return IDE_FAILURE;
}

IDE_API_ENTRY ide_handle IDE_API_CALL ideCreateInference(const char * binaryFilename)
{
    return NULL;
}

IDE_API_ENTRY ide_status IDE_API_CALL ideSetInput(ide_handle handle, void * inp_tensor_mem)
{
    return IDE_FAILURE;
}

IDE_API_ENTRY ide_status IDE_API_CALL ideGetOutput(ide_handle handle, void * out_tensor_mem)
{
    return IDE_FAILURE;
}

IDE_API_ENTRY ide_status IDE_API_CALL ideProcessInference(ide_handle handle)
{
    return IDE_FAILURE;
}

IDE_API_ENTRY ide_status IDE_API_CALL ideScheduleInference(ide_handle handle)
{
    return IDE_FAILURE;
}

IDE_API_ENTRY ide_status IDE_API_CALL ideWaitForCompletion(ide_handle handle)
{
    return IDE_FAILURE;
}

IDE_API_ENTRY ide_status IDE_API_CALL ideReleaseInference(ide_handle handle)
{
    return IDE_FAILURE;
}

""")

        else:
            input_shape = graph.inputs[0].shape
            output_shape = graph.outputs[0].shape
            input_buf_size = eval('*'.join([str(v) for v in input_shape])) * 4
            output_buf_size = eval('*'.join([str(v) for v in output_shape])) * 4
            config = 'input,' + graph.inputs[0].name + ',' + ','.join(str(v) for v in input_shape) + ';' + \
                     'output,' + graph.outputs[0].name + ',' + ','.join(str(v) for v in output_shape)
            f.write( \
"""
#include "idemodule.h"
#include <VX/vx.h>
#include <VX/vx_khr_nn.h>
#include <vx_amd_nn.h>
#include <vx_ext_amd.h>
#include <stdio.h>
#include <string.h>
#include <string>

#define ERROR_CHECK_OBJECT(obj) { vx_status status = vxGetStatus((vx_reference)(obj)); if(status != VX_SUCCESS) { vxAddLogEntry((vx_reference)context, status     , "ERROR: failed with status = (%%d) at " __FILE__ "#%%d\\n", status, __LINE__); return status; } }
#define ERROR_CHECK_STATUS(call) { vx_status status = (call); if(status != VX_SUCCESS) { vxAddLogEntry((vx_reference)context, status, "ERROR: failed with status = (%%d) at " __FILE__ "#%%d\\n", status, __LINE__); return status; } }

static void VX_CALLBACK log_callback(vx_context context, vx_reference ref, vx_status status, const vx_char string[])
{
    size_t len = strlen(string);
    if (len > 0) {
        printf("%%s", string);
        if (string[len - 1] != '\\n')
            printf("\\n");
        fflush(stdout);
    }
}

VX_API_ENTRY vx_status VX_API_CALL ideAddToGraph(vx_graph graph, %s, %s, const char * binaryFilename)
{
    vx_context context = vxGetContext((vx_reference)graph);
    ERROR_CHECK_OBJECT(context);
    ERROR_CHECK_STATUS(vxLoadKernels(context, "vx_nn"));

    // create variables
""" % (', '.join(['vx_tensor ' + tensor.name for tensor in graph.inputs]), \
       ', '.join(['vx_tensor ' + tensor.name for tensor in graph.outputs])))
        for tensor in graph.initializers:
            f.write( \
"""    vx_size dims_%s[%d] = { %s };
    vx_tensor %s = vxCreateTensor(context, %d, dims_%s, %s, 0);
    ERROR_CHECK_OBJECT(%s);
""" %(tensor.name, len(tensor.shape), ', '.join([str(v) for v in reversed(tensor.shape)]), \
      tensor.name, len(tensor.shape), tensor.name, tensor_type_nnir2openvx[tensor.type], tensor.name))
        f.write( \
"""
    // initialize variables
    FILE * fp__variables = fopen(binaryFilename, "rb");
    if(!fp__variables) {
        vxAddLogEntry((vx_reference)context, VX_FAILURE, "ERROR: unable to open: %s\\n", binaryFilename);
        return VX_FAILURE;
    }
    { vx_uint32 magic = 0;
      fread(&magic, 1, sizeof(magic), fp__variables);
      if(magic != 0xf00dd1e0) {
        vxAddLogEntry((vx_reference)context, VX_FAILURE, "ERROR: invalid file magic in %s\\n", binaryFilename);
        return VX_FAILURE;
      }
    }
""")
        for tensor in graph.initializers:
            f.write( \
"""    ERROR_CHECK_STATUS(initializeTensor(context, %s, fp__variables, binaryFilename));
""" %(tensor.name))
        f.write( \
"""    { vx_uint32 magic = 0;
      fread(&magic, 1, sizeof(magic), fp__variables);
      if(magic != 0xf00dd1e2) {
        vxAddLogEntry((vx_reference)context, VX_FAILURE, "ERROR: invalid eoff magic in %s\\n", binaryFilename);
        return VX_FAILURE;
      }
      fclose(fp__variables);
    }

    // create local tensors used in graph
""")
        localList = []
        for tensor in graph.locals:
            localList.append(tensor.name)
        outputList = []
        for tensor in graph.outputs:
            outputList.append(tensor.name)
        for idx, tensor in enumerate(graph.locals):
            if (not tensor.name in outputList) and (not tensor.name in localList[:idx]):
                f.write( \
"""    vx_size dims_%s[%d] = { %s };
    vx_tensor %s = vxCreateVirtualTensor(graph, %d, dims_%s, %s, 0);
    ERROR_CHECK_OBJECT(%s);
""" %(tensor.name, len(tensor.shape), ', '.join([str(v) for v in reversed(tensor.shape)]), \
      tensor.name, len(tensor.shape), tensor.name, tensor_type_nnir2openvx[tensor.type], tensor.name))
        f.write( \
"""
    // create nodes in graph
""")
        for node in graph.nodes:
            if node.type == 'conv':
                pads = node.attr.get('pads')
                dilations = node.attr.get('dilations')
                f.write( \
"""
    { vx_nn_convolution_params_t conv_params = { 0 };
      conv_params.padding_x = %d;
      conv_params.padding_y = %d;
      conv_params.overflow_policy = VX_CONVERT_POLICY_SATURATE;
      conv_params.rounding_policy = VX_ROUND_POLICY_TO_NEAREST_EVEN;
      conv_params.down_scale_size_rounding = VX_NN_DS_SIZE_ROUNDING_FLOOR;
      conv_params.dilation_x = %d;
      conv_params.dilation_y = %d;
      vx_node node = vxConvolutionLayer(graph, %s, %s, %s, &conv_params, sizeof(conv_params), %s);
      ERROR_CHECK_OBJECT(node);
""" % (pads[0], pads[1], dilations[0] - 1, dilations[1] - 1, \
      node.inputs[0], node.inputs[1], node.inputs[2] if len(node.inputs) == 3 else 'NULL', node.outputs[0]))
                if (node.attr.get('mode') != 0):
                    f.write( \
"""      vx_float32 alpha = 0;
      vx_scalar s_alpha = vxCreateScalarWithSize(context, VX_TYPE_FLOAT32, &alpha, sizeof(alpha));
      ERROR_CHECK_STATUS(vxSetParameterByIndex(node, 5, (vx_reference) s_alpha));
      ERROR_CHECK_STATUS(vxReleaseScalar(&s_alpha));
""")
                f.write( \
"""      ERROR_CHECK_STATUS(vxReleaseNode(&node));
    }
""")
            elif node.type == 'conv_transpose':
                pads = node.attr.get('pads')
                dilations = node.attr.get('dilations')
                kernel_shape = node.attr.get('kernel_shape')
                output_pads = [(dilations[0] - 1) * (kernel_shape[0] - 1), \
                                (dilations[1] - 1) * (kernel_shape[1] - 1)]
                f.write( \
"""
    { vx_nn_deconvolution_params_t conv_params = { 0 };
      conv_params.padding_x = %d;
      conv_params.padding_y = %d;
      conv_params.overflow_policy = VX_CONVERT_POLICY_SATURATE;
      conv_params.rounding_policy = VX_ROUND_POLICY_TO_NEAREST_EVEN;
      conv_params.a_x = %d;
      conv_params.a_y = %d;
      vx_node node = vxDeconvolutionLayer(graph, %s, %s, %s, &conv_params, sizeof(conv_params), %s);
      ERROR_CHECK_OBJECT(node);
      ERROR_CHECK_STATUS(vxReleaseNode(&node));
    }
""" % (pads[0], pads[1], output_pads[0] , output_pads[1] , \
      node.inputs[0], node.inputs[1], node.inputs[2] if len(node.inputs) == 3 else 'NULL', node.outputs[0]))
            elif node.type == 'gemm':
                alpha = node.attr.get('alpha')
                beta = node.attr.get('beta')
                transA = node.attr.get('transA')
                transB = node.attr.get('transB')
                hasBias = False
                if beta == 1.0 and len(node.inputs) == 3 and len(graph.tensor_shapes[node.inputs[2]]) <= 2:
                    hasBias = True
                if alpha == 1.0 and transA == 0 and transB == 1 and (beta == 0.0 or hasBias):
                    f.write( \
"""
    { vx_node node = vxFullyConnectedLayer(graph, %s, %s, %s, VX_CONVERT_POLICY_SATURATE, VX_ROUND_POLICY_TO_NEAREST_EVEN, %s);
      ERROR_CHECK_OBJECT(node);
      ERROR_CHECK_STATUS(vxReleaseNode(&node));
    }
""" % ( \
        node.inputs[0], node.inputs[1], node.inputs[2] if hasBias else 'NULL', node.outputs[0]))
                else:
                    raise ValueError("Unsupported gemm configuration by OpenVX: alpha={} beta={} transA={} transB={}".format(alpha, beta, transA, transB))
            elif node.type == 'max_pool' or node.type == 'avg_pool':
                f.write( \
"""
    { vx_node node = vxPoolingLayer(graph, %s, %s, %d, %d, %d, %d, VX_ROUND_POLICY_TO_NEAREST_EVEN, %s);
      ERROR_CHECK_OBJECT(node);
      vx_enum border_mode = %d;
      vx_scalar s_border_mode = vxCreateScalarWithSize(context, VX_TYPE_ENUM, &border_mode, sizeof(border_mode));
      ERROR_CHECK_OBJECT(s_border_mode);
      ERROR_CHECK_STATUS(vxSetParameterByIndex(node, 8, (vx_reference) s_border_mode));
      ERROR_CHECK_STATUS(vxReleaseScalar(&s_border_mode));
""" % (node.inputs[0], 'VX_NN_POOLING_AVG' if node.type == 'avg_pool' else 'VX_NN_POOLING_MAX', \
       node.attr.get('kernel_shape')[0], node.attr.get('kernel_shape')[1], \
       node.attr.get('pads')[0], node.attr.get('pads')[1], node.outputs[0], \
       (1 if node.attr.get('border_mode') == 'discard' else 0)))
                if (node.attr.get('mode') != 0):
                    f.write( \
"""      vx_int32 mode = %s;
      vx_scalar s_mode = vxCreateScalarWithSize(context, VX_TYPE_INT32, &mode, sizeof(mode));
      ERROR_CHECK_STATUS(vxSetParameterByIndex(node, 9, (vx_reference) s_mode));
      ERROR_CHECK_STATUS(vxReleaseScalar(&s_mode));
""" % (node.attr.get('mode')))
                f.write( \
"""      ERROR_CHECK_STATUS(vxReleaseNode(&node));
    }
""")
            elif node.type == 'global_avg_pool':
                f.write( \
"""
    { vx_node node = vxPoolingLayer(graph, %s, VX_NN_POOLING_AVG, %d, %d, %d, %d, VX_ROUND_POLICY_TO_NEAREST_EVEN, %s);
      ERROR_CHECK_OBJECT(node);
""" % (node.inputs[0], graph.tensor_shapes[node.inputs[0]][2], graph.tensor_shapes[node.inputs[0]][3], \
       node.attr.get('pads')[0], node.attr.get('pads')[1], node.outputs[0]))
                if (node.attr.get('mode') != 0):
                    f.write( \
"""      vx_int32 mode = %s;
      vx_scalar s_mode = vxCreateScalarWithSize(context, VX_TYPE_INT32, &mode, sizeof(mode));
      ERROR_CHECK_STATUS(vxSetParameterByIndex(node, 9, (vx_reference) s_mode));
      ERROR_CHECK_STATUS(vxReleaseScalar(&s_mode));
""" % (node.attr.get('mode')))
                f.write( \
"""      ERROR_CHECK_STATUS(vxReleaseNode(&node));
    }
""")
            elif node.type == 'relu':
                f.write( \
"""
    { vx_node node = vxActivationLayer(graph, %s, VX_NN_ACTIVATION_RELU, 0.0f, 0.0f, %s);
      ERROR_CHECK_OBJECT(node);
      ERROR_CHECK_STATUS(vxReleaseNode(&node));
    }
""" % (node.inputs[0], node.outputs[0]))
            elif node.type == 'leaky_relu':
                f.write( \
"""
    {  vx_node node = vxActivationLayer(graph, %s, VX_NN_ACTIVATION_LEAKY_RELU, %f, 0.0f, %s);
       ERROR_CHECK_OBJECT(node);
       ERROR_CHECK_STATUS(vxReleaseNode(&node));
    }
""" % (node.inputs[0], node.attr.get('alpha'), node.outputs[0]))
            elif node.type == 'add' or node.type == 'sum':
                if len(node.inputs) == 2:
                    f.write( \
"""
    { vx_node node = vxTensorAddNode(graph, %s, %s, VX_CONVERT_POLICY_SATURATE, %s);
      ERROR_CHECK_OBJECT(node);
      ERROR_CHECK_STATUS(vxReleaseNode(&node));
    }
""" % (node.inputs[0], node.inputs[1], node.outputs[0]))
                else:
                    raise ValueError("Unsupported number of input arguments by OpenVX: {}".format(node.type))
            elif node.type == 'sub':
                if len(node.inputs) == 2:
                    f.write( \
"""
    { vx_node node = vxTensorSubtractNode(graph, %s, %s, VX_CONVERT_POLICY_SATURATE, %s);
      ERROR_CHECK_OBJECT(node);
      ERROR_CHECK_STATUS(vxReleaseNode(&node));
    }
""" % (node.inputs[0], node.inputs[1], node.outputs[0]))
                else:
                    raise ValueError("Unsupported number of input arguments by OpenVX: {}".format(node.type))
            elif node.type == 'mul':
                if len(node.inputs) == 2:
                    f.write( \
"""
    { vx_float32 value = 1.0f;
      vx_scalar scale = vxCreateScalar(context, VX_TYPE_FLOAT32, &value);
      ERROR_CHECK_OBJECT(scale);
      vx_node node = vxTensorMultiplyNode(graph, %s, %s, scale, VX_CONVERT_POLICY_SATURATE, VX_ROUND_POLICY_TO_NEAREST_EVEN, %s);
      ERROR_CHECK_STATUS(vxReleaseScalar(&scale));
      ERROR_CHECK_OBJECT(node);
      ERROR_CHECK_STATUS(vxReleaseNode(&node));
    }
""" % (node.inputs[0], node.inputs[1], node.outputs[0]))
                else:
                    raise ValueError("Unsupported number of input arguments by OpenVX: {}".format(node.type))
            elif node.type == 'muladd':
                tensor = graph.tensor_dict[node.inputs[0]]
                f.write( \
"""
    { vx_float32 value = 1.0f;
      vx_scalar scale = vxCreateScalar(context, VX_TYPE_FLOAT32, &value);
      ERROR_CHECK_OBJECT(scale);
      vx_size dims[%d] = { %s };
      vx_tensor tmp__tensor = vxCreateVirtualTensor(graph, %d, dims, %s, 0);
      ERROR_CHECK_OBJECT(tmp__tensor);
      vx_node node = vxTensorMultiplyNode(graph, %s, %s, scale, VX_CONVERT_POLICY_SATURATE, VX_ROUND_POLICY_TO_NEAREST_EVEN, tmp__tensor);
      ERROR_CHECK_STATUS(vxReleaseScalar(&scale));
      ERROR_CHECK_OBJECT(node);
      ERROR_CHECK_STATUS(vxReleaseNode(&node));
      node = vxTensorAddNode(graph, tmp__tensor, %s, VX_CONVERT_POLICY_SATURATE, %s);
      ERROR_CHECK_OBJECT(node);
      ERROR_CHECK_STATUS(vxReleaseNode(&node));
    }
""" % (len(tensor.shape), ', '.join([str(v) for v in reversed(tensor.shape)]), len(tensor.shape), \
       tensor_type_nnir2openvx[tensor.type], node.inputs[0], node.inputs[1], node.inputs[2], node.outputs[0]))
            elif node.type == 'batch_norm':
                f.write( \
"""
    { vx_node node = vxBatchNormalizationLayer(graph, %s, %s, %s, %s, %s, %ef, %s);
      ERROR_CHECK_OBJECT(node);
      ERROR_CHECK_STATUS(vxReleaseNode(&node));
    }
""" % (node.inputs[0], node.inputs[3], node.inputs[4], node.inputs[1], node.inputs[2], node.attr.get('epsilon'), node.outputs[0]))
            elif node.type == 'lrn':
                f.write( \
"""
    { vx_node node = vxNormalizationLayer(graph, %s, %s , %d, %ef, %ef, %s);
""" % (node.inputs[0], "VX_NN_NORMALIZATION_SAME_MAP" if node.attr.get('mode') == 0 else "VX_NN_NORMALIZATION_ACROSS_MAPS" , \
       node.attr.get('size'), node.attr.get('alpha'), node.attr.get('beta'), node.outputs[0]))
                if (node.attr.get('bias') != 1.0):
                    f.write( \
"""   vx_float32 bias = %s;
      vx_scalar s_bias = vxCreateScalarWithSize(context, VX_TYPE_FLOAT32, &bias, sizeof(bias));
      ERROR_CHECK_STATUS(vxSetParameterByIndex(node, 6, (vx_reference) s_bias));
      ERROR_CHECK_STATUS(vxReleaseScalar(&s_bias));
""" % (node.attr.get('bias')))
                f.write( \
"""   ERROR_CHECK_OBJECT(node);
      ERROR_CHECK_STATUS(vxReleaseNode(&node));
    }
""")                        
            elif node.type == 'slice':
                f.write( \
"""
    { vx_node node = vxSliceLayer(graph, %s, %s, %s);
      ERROR_CHECK_OBJECT(node);
      ERROR_CHECK_STATUS(vxReleaseNode(&node));
    }
""" % (node.inputs[0], ', '.join([name for name in node.outputs]), \
       ', '.join(['NULL' for i in range(8 - len(node.outputs))])))
            elif node.type == 'concat':
                f.write( \
"""
    { vx_node node = vxConcatLayer(graph, %s, %s, %s);
      ERROR_CHECK_OBJECT(node);
      ERROR_CHECK_STATUS(vxReleaseNode(&node));
    }
""" % (node.outputs[0], ', '.join([name for name in node.inputs]), \
       ', '.join(['NULL' for i in range(8 - len(node.inputs))])))
            elif node.type == 'softmax':
                f.write( \
"""
    { vx_node node = vxSoftmaxLayer(graph, %s, %s);
      ERROR_CHECK_OBJECT(node);
      ERROR_CHECK_STATUS(vxReleaseNode(&node));
    }
""" % (node.inputs[0], node.outputs[0]))
            elif node.type == 'reshape':
                f.write( \
"""
    { vx_node node = vxReshapeLayer(graph, %s, %s);
      ERROR_CHECK_OBJECT(node);
      ERROR_CHECK_STATUS(vxReleaseNode(&node));
    }
""" % (node.inputs[0], node.outputs[0]))
            elif node.type == 'copy'or node.type == 'transpose':
                f.write( \
"""
    { vx_node node = vxCopyNode(graph, %s, %s);
      ERROR_CHECK_OBJECT(node);
      ERROR_CHECK_STATUS(vxReleaseNode(&node));
    }
""" % (node.inputs[0], node.outputs[0]))
            else:
                raise ValueError("Unsupported node by OpenVX: {}".format(node.type))
        f.write( \
"""
    // release local tensors
""")
        for idx, tensor in enumerate(graph.locals):
            if (not tensor.name in outputList) and (not tensor.name in localList[:idx]):
                f.write( \
"""    ERROR_CHECK_STATUS(vxReleaseTensor(&%s));
""" %(tensor.name))
        f.write( \
"""
    // release initializer tensors
""")
        for tensor in graph.initializers:
            f.write( \
"""    ERROR_CHECK_STATUS(vxReleaseTensor(&%s));
""" %(tensor.name))
        f.write( \
"""
    return VX_SUCCESS;
}

IDE_API_ENTRY ide_status IDE_API_CALL ideQueryInference(char **inp_name, size_t *inp_dims, char **out_name, size_t *out_dims)
{
    *inp_name = %s;
    inp_dims  = { %s };
    *out_name = %s;
    out_dims = { %s };
    return IDE_SUCCESS;
}
""" %(graph.inputs[0].name, ', '.join([str(v) for v in reversed(input_shape)]), graph.outputs[0].name, \
      ', '.join([str(v) for v in reversed(input_shape)])))
        f.write( \
"""
IDE_API_ENTRY ide_status IDE_API_CALL ideSetBackend(const char *nameBackend)
{
    return IDE_ERROR_NOT_IMPLEMENTED;
}

IDE_API_ENTRY ide_handle IDE_API_CALL ideCreateInference(const char * binaryFilename, int mem_type)
{
    bool successful = false;

    ide_handle handle = new ide_handle();
    memset(handle, 0, sizeof(ide_handle));
    if(!handle) {
        printf("ERROR: new ide_handle: failed (nullptr)\\n");
    }
    else {
        vx_status status;
        vxRegisterLogCallback(NULL, log_callback, vx_false_e);
        handle->context = vxCreateContext();
        if((status = vxGetStatus((vx_reference)handle->context)) != VX_SUCCESS) {
            printf("ERROR: vxCreateContext: failed (%%d)\\n", status);
        }
        else {
            vxRegisterLogCallback(context, log_callback, vx_false_e);
            handle->graph = vxCreateGraph(handle->context);
            if((status = vxGetStatus((vx_reference)handle->graph)) != VX_SUCCESS) {
                printf("ERROR: vxCreateGraph: failed (%%d)\\n", status);
            }
            else {
                vx_size inp_dim[4] = { %s };
                vx_size inp_stride[4] = { 4, (vx_size)4 * inp_dim[0], (vx_size)4 * inp_dim[0] * inp_dim[1], (vx_size)4 * inp_dim[0] * inp_dim[1] * inp_dim[2]}
                if (!mem_type) {
                    handle->input = vxCreateTensorFromHandle(handle->context, 4, inp_dim, VX_TYPE_FLOAT32, 0, inp_stride, inp_mem, VX_MEMORY_TYPE_HOST);
                }
                else {
                    cl_mem inp_mem = nullptr;
                    handle->input = vxCreateTensorFromHandle(handle->context, 4, inp_dim, VX_TYPE_FLOAT32, 0, inp_stride, inp_mem, VX_MEMORY_TYPE_OPENCL);
                }
                if((status = vxGetStatus((vx_reference)handle->input)) != VX_SUCCESS) {
                    printf("ERROR: vxCreateTensor(input:[%s]): failed (%%d)\\n", status);
                else {
                    vx_size out_dim[%d] = { %s };
                    handle->output = vxCreateTensor(handle->graph, %d, out_dim, VX_TYPE_FLOAT32, 0);
                    if((status = vxGetStatus((vx_reference)handle->output)) != VX_SUCCESS) {
                        printf("ERROR: vxCreateTensor(output:[%s]): failed (%%d)\\n", status);
                    }
                    else if((status = ideAddToGraph(handle->graph, handle->input, handle->output, binaryFilename)) != VX_SUCCESS) {
                        printf("ERROR: ideAddToGraph: failed (%%d)\\n", status);
                    }
                    else if((status = vxVerifyGraph(handle->graph)) != VX_SUCCESS) {
                        printf("ERROR: vxVerifyGraph: failed (%%d)\\n", status);
                    }
                    else {
                        printf("OK: ideCreateInference: successful\\n");
                        successful = true;
                    }
                }
            }
        }
    }

    if(!successful) {
        if(handle) {
            if(handle->graph)
                vxReleaseGraph(&handle->graph);
            if(handle->input)
                vxReleaseTensor(&handle->input);
            if(handle->output)
                vxReleaseTensor(&handle->output);
            if(handle->context)
                vxReleaseContext(&handle->context);
            delete handle;
            handle = nullptr;
        }
    }

    return handle;
}
""" % (', '.join([str(v) for v in reversed(input_shape)]), 'x'.join([str(v) for v in input_shape]), len(output_shape), \
       ', '.join([str(v) for v in reversed(output_shape)]), len(output_shape), 'x'.join([str(v) for v in output_shape])))

        f.write( \
"""

IDE_API_ENTRY ide_status IDE_API_CALL ideSetInput(ide_handle handle, void * inp_tensor_mem)
{
    // create local tensors used in graph: mem_type = 0 for CPU buffer and 1 for OpenCL
    vx_status status = VX_SUCCESS;
    if (!inp_tensor_mem) {
        printf("ERROR: ideSetInput: invalid input memory pointer\\n", status);
        return 1;
    }else {
        // application is passing device memory pointer, do SwapTensorHandle()
        if ((status = vxSwapTensorHandle(handle->input, input_tensor_mem, nullptr)) != VX_SUCCESS) {
            printf("ERROR: ideSetInput: vxSwapTensorHandle: failed (%%d)\\n", status);
        }
    }
    return status;
}

IDE_API_ENTRY int IDE_API_CALL ideGetOutput(ide_handle handle, void * out_tensor_mem)
{
    vx_status status = VX_SUCCESS;
    vx_size stride[4] = { 4, %d, %d, %d };
    if(!handle) {
        status = VX_FAILURE;
        printf("ERROR: ideGetOutput: invalid handle\\n");
    }
    else if(!out_tensor_mem) {
        status = VX_FAILURE;
        printf("ERROR: ideGetOutput: invalid output buffer %p\\n", out_tensor_mem);
    }
    else if(handle->output && (status = vxCopyTensorPatch(handle->output, %d, nullptr, nullptr, stride, out_tensor_mem, VX_READ_ONLY, VX_MEMORY_TYPE_HOST)) != VX_SUCCESS) {
        printf("ERROR: ideGetOutput: vxCopyTensorPatch: failed (%%d)\\n", status);
    }
    return status;
}
""" % (tshape[3]*4, tshape[2]*tshape[3]*4, tshape[1]*tshape[2]*tshape[3]*4, len(output_shape)))

        f.write( \
"""
IDE_API_ENTRY int IDE_API_CALL ideProcessInference(ide_handle handle)
{
    vx_status status = VX_SUCCESS;
    if(!handle) {
        status = VX_FAILURE;
        printf("ERROR: ideProcessInference: invalid handle\\n");
    }
    else if (status = vxProcessGraph(handle->graph)){
            printf("ERROR: ideGetOutput: vxProcessGraph: failed (%%d)\\n", status);
        }
    }
    return status;
}

IDE_API_ENTRY int IDE_API_CALL ideScheduleInference(ide_handle handle)
{
    vx_status status = VX_SUCCESS;
    if(!handle) {
        status = VX_FAILURE;
        printf("ERROR: ideScheduleInference: invalid handle\\n");
    }
    else {
            if (status = vxScheduleGraph(handle->graph)) == VX_SUCCESS)
                handle->scheduled = true;
            else
                printf("ERROR: ideScheduleInference: failed (%%d)\\n", status);
        }
    }
    return status;
}

IDE_API_ENTRY int IDE_API_CALL ideWaitForCompletion(ide_handle handle)
{
    vx_status status = VX_SUCCESS;
    if(!handle) {
        status = VX_FAILURE;
        printf("ERROR: ideWaitForCompletion: invalid handle\\n");
    }
    else {
            if (status = vxWaitGraph(handle->graph)) == VX_SUCCESS)
                handle->scheduled = false;
            else
                printf("ERROR: ideWaitForCompletion: failed\\n");
        }
    }
    return status;
}

IDE_API_ENTRY int IDE_API_CALL ideReleaseInference(ide_handle handle)
{
    vx_status status = VX_SUCCESS;
    if(!handle) {
        status = VX_FAILURE;
        printf("ERROR: ideReleaseInference: invalid handle\\n");
    }
    else if(handle->graph && (status = vxReleaseGraph(&handle->graph)) != VX_SUCCESS) {
        printf("ERROR: ideReleaseInference: vxReleaseGraph: failed (%d)\\n", status);
    }
    else if(handle->input && (status = vxReleaseTensor(&handle->input)) != VX_SUCCESS) {
        printf("ERROR: ideReleaseInference: vxReleaseTensor(input): failed (%d)\\n", status);
    }
    else if(handle->output && (status = vxReleaseTensor(&handle->output)) != VX_SUCCESS) {
        printf("ERROR: ideReleaseInference: vxReleaseTensor(output): failed (%d)\\n", status);
    }
    else if(handle->context && (status = vxReleaseContext(&handle->context)) != VX_SUCCESS) {
        printf("ERROR: ideReleaseInference: vxReleaseContext: failed (%d)\\n", status);
    }
    else {
        delete handle;
    }
    return status;
}

""")

def generateBinary(graph,fileName):
    VARIABLES_FILE_MAGIC = 0xF00DD1E0
    VARIABLES_DATA_MAGIC = 0xF00DD1E1
    VARIABLES_EOFF_MAGIC = 0xF00DD1E2
    print('creating ' + fileName + ' ...')
    with open(fileName, 'wb') as f:
        f.write(struct.pack('I', VARIABLES_FILE_MAGIC))
        for tensor in graph.initializers:
            binary = graph.binaries[tensor.name]
            f.write(struct.pack('II', VARIABLES_DATA_MAGIC, len(binary)))
            f.write(binary)
        f.write(struct.pack('I', VARIABLES_EOFF_MAGIC))

def generateTestCPP(graph,argmaxOutput,fileName):        
    print('creating ' + fileName + ' ...')
    with open(fileName, 'w') as f:
        generateLicenseForCPP(f)
        f.write( \
"""
#include "idemodule.h"
#include <iostream>
#include <sstream>
#include <vector>
#include <stdio.h>
#include <string.h>
#include <string>
#include <inttypes.h>
#if ENABLE_OPENCV
#include <opencv2/opencv.hpp>
#include <opencv/cv.h>
#include <opencv/highgui.h>
using namespace cv;
#endif

extern "C" {
    typedef IDE_API_ENTRY int (VX_API_CALL *ideQueryInference_t)(char **inp_name, size_t *inp_dims, char **out_name, size_t *out_dims);
    typedef IDE_API_ENTRY int (VX_API_CALL *ideSetBackend_t)();
    typedef IDE_API_ENTRY ide_handle (VX_API_CALL *ideCreateInference_t)(void *input, void *output, const char * binaryFilename);
    typedef IDE_API_ENTRY int  (VX_API_CALL *ideSetInput_t)(ide_handle handle, void * inp_tensor_mem);
    typedef IDE_API_ENTRY int  (VX_API_CALL *ideGetOutput_t)(ide_handle handle, void * out_tensor_mem);
    typedef IDE_API_ENTRY int  (VX_API_CALL *ideProcessInference_t)(ide_handle handle, int num_iterations = 1);
    typedef IDE_API_ENTRY int  (VX_API_CALL *ideScheduleInference_t)(ide_handle handle);
    typedef IDE_API_ENTRY int  (VX_API_CALL *ideWaitForCompletion_t)(ide_handle handle);
    typedef IDE_API_ENTRY int  (VX_API_CALL *ideReleaseInference_t)(ide_handle handle);
};

class InferenceDeployAPI
{

    void *libHandle;
    ideQueryInference_t     ideQueryInference_f;
    ideSetBackend_t         ideSetBackend_f;
    ideCreateInference_t    ideCreateInference_f;
    ideSetInput_t           ideSetInput_f;
    ideGetOutput_t          ideGetOutput_f;
    ideProcessInference_t   ideProcessInference_f;
    ideScheduleInference_t  ideScheduleInference_f;
    ideWaitForCompletion_t  ideWaitForCompletion_f;
    ideReleaseInference_t   ideReleaseInference_f;


public: 
    InferenceDeployAPI(const char *library_name);
    ~InferenceDeployAPI();
}

InferenceDeployAPI::InferenceDeployAPI(const char *library_name)
{
    libHandle = dlopen(library_name, RTLD_NOW | RTLD_LOCAL);
    if (!libHandle)
    {
        printf("ERROR: couldn't load inference deployment lib \\n");     
    }
    ideQueryInference_f     = (ideQueryInference_t) dlsym(libHandle, "ideQueryInference");
    ideSetBackend_f         = (ideSetBackend_t) dlsym(libHandle, "ideSetBackend");
    ideCreateInference_f    = (ideCreateInference_t) dlsym(libHandle, "ideQueryInference");
    ideSetInput_f           = (ideSetInput_t) dlsym(libHandle, "ideSetInput");
    ideGetOutput_f          = (ideGetOutput_t) dlsym(libHandle, "ideGetOutput");
    ideProcessInference_f   = (ideProcessInference_t) dlsym(libHandle, "ideProcessInference");
    ideScheduleInference_f  = (ideScheduleInference_t) dlsym(libHandle, "ideScheduleInference");
    ideWaitForCompletion_f  = (ideWaitForCompletion_t) dlsym(libHandle, "ideWaitForCompletion");
    ideReleaseInference_f   = (ideReleaseInference_t) dlsym(libHandle, "ideReleaseInference");
    if (!ideQueryInference_f)
    {
        printf("ERROR: couldn't find function ideQueryInference in module %%s \\n", library_name);      
    }
    if (!ideSetBackend_f)
    {
        printf("ERROR: couldn't find function ideSetBackend in module %%s \\n", library_name);      
    }
    if (!ideCreateInference_f)
    {
        printf("ERROR: couldn't find function ideQueryInference in module %%s \\n", library_name);      
    }
    if (!ideSetInput_f)
    {
        printf("ERROR: couldn't find function ideQueryInference in module %%s \\n", library_name);      
    }
    if (!ideGetOutput_f)
    {
        printf("ERROR: couldn't find function ideQueryInference in module %%s \n", library_name);      
    }
    if (!ideProcessInference_f)
    {
        printf("ERROR: couldn't find function ideProcessInference in module %%s \n", library_name);      
    }
    if (!ideScheduleInference_f)
    {
        printf("ERROR: couldn't find function ideScheduleInference in module %%s \n", library_name);      
    }
    if (!ideWaitForCompletion_f)
    {
        printf("ERROR: couldn't find function ideWaitForCompletion in module %%s \n", library_name);      
    }
    if (!ideReleaseInference)
    {
        printf("ERROR: couldn't find function ideReleaseInference in module %%s \n", library_name);      
    }
}

int main(int argc, const char ** argv)
{
    // check command-line usage
    if(argc < 4) {
        printf("Usage: infdeploy <infdeploylib> <weightsfile> [[<input_data_file(s)> [<output_data_file(s)>]] -t <for performance>\\n"
            "   <infdeploylib>: is the name of the inference deploy lib(.so)\\n"
            "   <weightsfile> : is the name of the bin weights file\\n"
            "   <input-data-file>: is filename(s) to initialize input tensor\\n"
            "   <output-data-file>: is filename to initialize output tensor\\n"
""")
        f.write( \
"""#if ENABLE_OPENCV
            "     .jpg or .png: decode and copy to raw_data file\\n"
#endif
""")
        f.write( \
"""            "     other: initialize tensor with raw data from the file\\n"
            "\\n"
""")
        if type(argmaxOutput) is str:
            f.write( \
"""             "   <output-data-file>[,<reference-for-compare>,<percentMismatchLimit>]:\\n"
            "     <referece-to-compare> is raw tensor data of argmax output for comparision\\n"
            "     <percentMismatchLimit> is max mismatches (percent) allowed\\n"
            "     <output-data-file> is filename for saving output tensor data\\n"
""")
        else:
            f.write( \
"""             "   <output-data-file>[,<reference-for-compare>,<maxErrorLimit>,<rmsErrorLimit>]:\\n"
            "     <referece-to-compare> is raw tensor data for comparision\\n"
            "     <maxErrorLimit> is max absolute error allowed\\n"
            "     <rmsErrorLimit> is max RMS error allowed\\n"
            "     <output-data-file> is filename for saving output tensor data\\n"
            "\\n"
        );
        return -1;
    }
    ide_status status;
    size_t inp_dims[4], out_dims[4];
    std::string inpName, outName;
    float *inpMem = null_ptr;
    float *outMem = null_ptr;
    const char * infDeployLib = argv[1];
    const char * weightsFile  = argv[2];
    const char * inpFileName  = argv[3];
    const char * outFileName  = argv[4];
    int bPeformanceRun = 0; 
    if (argc > 4) &&!strcmp(argv[5], "-t")
        bPeformanceRun = 1;
    // Create InferenceDeploy Object
    InferenceDeployAPI *infDeploy = new InferenceDeployAPI(infdeploylib);

    if (infDeploy->ideQueryInference_f(&inpName.cstr(), inp_dims, &outName.cstr(), out_dims))
    {
        printf("ERROR: ideQueryInference_f returned failure \n");      
    }
    else {
        ide_handle infHandle = infDeploy->ideCreateInference_f(weightsFile, 0);
        if (!infHandle)
        {
            printf("ERROR: ideCreateInference_f returned failure \n");
            return -1;      
        }
        // create input tensor memory for swaphandle
        size_t inputSizeInBytes = 4 *inp_dims[0]*inp_dims[1]*inp_dims[2]*inp_dims[3];
        inpMem = (float *)new char[inputSizeInBytes];
        size_t istride[4] = { 4, (size_t)4 * inp_dims[0], (size_t)4 * inp_dims[0] * inp_dims[1], (size_t)4 * inp_dims[0] * inp_dims[1] * inp_dims[2] };

#if ENABLE_OPENCV
        if(inp_dims[2] == 3 && inpFileName.size() > 4 && (inpFileName.substr(fileName.size()-4, 4) == ".png" || inpFileName.substr(inpFileName.size()-4, 4) == ".jpg"))
        {
            for(size_t n = 0; n < inp_dims[3]; n++) {
                char imgFileName[1024];
                sprintf(imgFileName, inpFileName, (int)n);
                Mat img = imread(imgFileName, CV_LOAD_IMAGE_COLOR);
                if(!img.data || img.rows != inp_dims[1] || img.cols != inp_dims[0]) {
                    printf("ERROR: invalid image or dimensions: %s\\n", imgFileName);
                    return -1;
                }
                for(vx_size y = 0; y < inp_dims[1]; y++) {
                    unsigned char * src = img.data + y*inp_dims[0]*3;
                    float * dstR = inpMem + ((n * istride[3] + y * istride[1]) >> 2);
                    float * dstG = dstR + (istride[2] >> 2);
                    float * dstB = dstG + (istride[2] >> 2);
                    for(vx_size x = 0; x < inp_dims[0]; x++, src += 3) {
                        *dstR++ = src[2];
                        *dstG++ = src[1];
                        *dstB++ = src[0];
                    }
                }
            }
        }
        else
#endif
        {
            FILE * fp = fopen(inpFileName, "rb");
            if(!fp) {
                std::cerr << "ERROR: unable to open: " << inpFileName << std::endl;
                return -1;
            }
            for(size_t n = 0; n < dims[3]; n++) {
                for(size_t c = 0; c < dims[2]; c++) {
                    for(size_t y = 0; y < dims[1]; y++) {
                        float * ptrY = inpMem + ((n * istride[3] + c * istride[2] + y * istride[1]) >> 2);
                        vx_size n = fread(ptrY, sizeof(float), dims[0], fp);
                        if(n != dims[0]) {
                            std::cerr << "ERROR: expected char[" << count*sizeof(float) << "], but got less in " << inpFileName << std::endl;
                            return -1;
                        }
                    }
                }
            }
            fclose(fp);
        }
        if (status = infDeploy->ideSetInput_f(ide_handle, inpMem)) {
            printf("ERROR: ideSetInput_f returned failure(%%d) \n", status);
            return -1;
        }
        if (status = infDeploy->ideProcessInference_f(ide_handle)) {
            printf("ERROR: ideProcessInference_f returned failure(%%d) \n", status);
            return -1;
        }
        // allocate output tensor
        float *inpMem = null_ptr;
        size_t outputSizeInBytes = 4 *out_dims[0]*out_dims[1]*out_dims[2]*out_dims[3];
        outMem = (float *)new char[inputSizeInBytes];

        // get output
        if (status = infDeploy->ideGetOutput_t(ide_handle, outMem)) {
            printf("ERROR: ideProcessInference_f returned failure(%%d) \n", status);
            return -1;
        }
        if (strcmp(outFileName, "-") != 0)
        {
            FILE *fp = fopen(outFileName, "wb");
              if(!fp) {
                std::cerr << "ERROR: unable to open: " << outFileName << std::endl;
                return -1;
            }
            fwrite(ptr, sizeof(float), outputSizeInBytes>>2, fp);
            fclose(fp);
        }

        if (bPeformanceRun) {
            t0 = clockCounter();
            int N = 100;
            for(int i = 0; i < N; i++) {
                status = infDeploy->ideProcessInference_f(ide_handle);
                if(status != IDE_SUCCESS)
                    break;
            }
            t1 = clockCounter();
            printf("OK: ideProcessInference() took %.3f msec (average over %d iterations)\\n", (float)(t1-t0)*1000.0f/(float)freq/(float)N, N);
        }
        // Relese Inference
        infDeploy->ideReleaseInference_f(ide_handle);
        printf("OK: Inference Deploy Successful\n");
        // delete resources
        if (inpMem) delete[] inpMem;
        if (outMem) delete[] outMem;
        if (infDeploy) delete infDeploy;
    }
}
""")

def generateCode(graph,argmaxOutput,outputFolder):
    if not os.path.isdir(outputFolder):
        os.mkdir(outputFolder)
    generateCMakeFiles(graph,outputFolder)
    generateModuleH(graph,outputFolder + '/idemodule.h')
    generateModuleCPP(graph,outputFolder + '/idemodule.cpp')
    generateBinary(graph,outputFolder + '/weights.bin')
    generateTestCPP(graph,argmaxOutput,outputFolder + '/infdeploy.cpp')

def main():
    usage = """
Usage: python nnir2clib_vx.py [OPTIONS] <nnirInputFolder> <outputFolder>

  OPTIONS:
    --argmax UINT8                    -- argmax at the end with 8-bit output
    --argmax UINT16                   -- argmax at the end with 16-bit output
    --argmax <fileNamePrefix>rgb.txt  -- argmax at the end with RGB color mapping using LUT
    --argmax <fileNamePrefix>rgba.txt -- argmax at the end with RGBA color mapping using LUT
    --help                            -- show this help message

  LUT File Format (RGB): 8-bit R G B values one per each label in text format
    R0 G0 B0
    R1 G1 B1
    ...

  LUT File Format (RGBA): 8-bit R G B A values one per each label in text format
    R0 G0 B0 A0
    R1 G1 B1 A1
    ...

"""
    pos = 1;
    argmaxOutput = None
    while len(sys.argv[pos:]) >= 2 and sys.argv[pos][:2] == '--':
        if sys.argv[pos] == '--argmax':
            argmaxOutput = sys.argv[pos+1]
            if argmaxOutput == 'UINT8':
                argmaxOutput = 'vx_uint8'
            elif argmaxOutput == 'UINT16':
                argmaxOutput = 'vx_uint16'
            else:
                if not os.path.isfile(argmaxOutput):
                    print('ERROR: unable to open: %s' % (argmaxOutput))
                    sys.exit(1)
                with open(argmaxOutput,'r') as f:
                    if argmaxOutput[-8:] == 'rgba.txt':
                        argmaxOutput = np.reshape(np.array([int(v) for v in f.read().split()]), [-1, 4]).transpose()
                    else:
                        argmaxOutput = np.reshape(np.array([int(v) for v in f.read().split()]), [-1, 3]).transpose()
        else:
            if sys.argv[pos] != '--help':
                print('ERROR: invalid option: %s' % (sys.argv[pos]))
            print(usage)
            sys.exit(1)
        pos = pos + 2
    if len(sys.argv[pos:]) < 2:
        print(usage)
        sys.exit(1)
    inputFolder = sys.argv[pos]
    outputFolder = sys.argv[pos+1]
    print('reading IR model from ' + inputFolder + ' ...')
    graph = IrGraph()
    graph.fromFile(inputFolder)
    print('creating C code in ' + outputFolder + ' ...')
    generateCode(graph,argmaxOutput,outputFolder)

if __name__ == '__main__':
    main()
